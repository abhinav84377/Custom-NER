{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0fcfa8-905d-4e8a-bbc7-757988366b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef1403f-f381-4a52-a7dc-aed46a8ecf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f780b5-be92-482d-9acb-9b9f6afccff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Abhinav\\OneDrive\\Documents\\Git Projects\\Custom-NER\\NER_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bec7f4-58ee-42a3-8a27-c5d59f19dacf",
   "metadata": {},
   "source": [
    "## Types of NER training\n",
    "\n",
    "### 1. Dictionary Based: \n",
    "This is the simplest NER approach. Here we will be having a dictionary that contains a collection of vocabulary. In this approach, basic string matching algorithms are used to check whether the entity is occurring in the given text to the items in the vocabulary. The method has limitations as it is required to update and maintain the dictionary used for the system.\r\n",
    "### 2. Rule-based Systems\r",
    "Here, the model uses a pre-defined set of rules for information extraction. Mainly two types of rules are used, Pattern-based rules, which depend upon the morphological pattern of the words used, and context-based rules, which depend upon the context of the word used in the given text document. A simple example for a context-based rule is “If a person’s title is followed by a proper noun, then that proper noun is the name of a person”.\n",
    "###### Tokenization, Dependency Parser Tree, Pos-Tags, Stemming etc\r\n",
    "### 3. ML Based System\n",
    "The ML-based systems use statistical-based models for detecting the entity names. These models try to make a feature-based representation of the observed data. By this approach, a lot of limitations of dictionary and rule-based approaches are solved by recognizing an existing entity name, even with small spelling variations.\r\n",
    "### 4. Deep Learning based\n",
    "In recent years, deep learning-based models are being used for building state-of-the-art systems for NER. There are many advantages of using DL techniques over the previously discussed approaches. Using the DL approach, the input data is mapped to a non-linear representation. This approach helps to learn complex relations that are present in the input data. Another advantage is that we can avoid a lot of time and resources spent on feature engineering, which is required for the other traditional approaches.\n",
    "\r\n",
    "\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecbf296-a2a3-4f38-9192-3d37667d3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb8d251-9a48-4599-84f7-65791935d13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f77552-5ce3-49ca-b633-dae651e05d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentence'] = data[\"Word\"].apply(lambda l: \" \".join(eval(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa78f4b-7df2-42b6-9534-17ccc35ea5eb",
   "metadata": {},
   "source": [
    "### Types of tag\n",
    "Let's have a look at an example. If you have a sentence like \\\n",
    "\"Barack Obama was born in Hawaï\", \\\n",
    "[B-PERS, I-PERS, O, O, O, B-GEO]. \\\n",
    "B-PERS means that the word \"Barack\" is the **beginning** of a person, \\\n",
    "I-PERS means that the word \"Obama\" is **inside** a person,\\\n",
    "\"O\" means that the word \"was\" is **outside a named entity**, and so on. So one typically has as many tags as there are words in a sentence.\\\r\n",
    "So if you want to train a deep learning model for NER, it requires that you have your data in this **IOB format(Inside-Outside-Beginning) (or similar formats such as BILOU)**. There exist many annotation tools which let you create these kind of annotations automatically (such as Spacy's Prodigy, Tagtog or Doccano). You can also use Spacy's biluo_tags_from_offsets function to convert annotations at the character level to IOB format.\r\n",
    "\r\n",
    "Here, we will use a NER dataset from Kaggle that is already in IOB format. One has to go to this web page, download the dataset, unzip it, and upload the csv file to this notebook. Let's print out the first few rows of this csv file:\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24835b0c-47ff-4c98-86af-e7625a59caed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tag\n",
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Tag\"] = data[\"Tag\"].apply(ast.literal_eval)\n",
    "frequencies= data[\"Tag\"].explode().value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80690fe9-c31f-4cbb-8926-aba734c58571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geo', 45058), ('org', 36927), ('per', 34241), ('tim', 26861), ('gpe', 16068), ('art', 699), ('eve', 561), ('nat', 252)]\n"
     ]
    }
   ],
   "source": [
    "#Checking Unique tags and count\n",
    "#we can remove nat, eve or art tags, as the count is very less for training\n",
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:5] not in tags.keys():\n",
    "            tags[tag[2:5]] = count\n",
    "        else:\n",
    "            tags[tag[2:5]] += count\n",
    "    continue\n",
    "\n",
    "print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f35b80-5d45-49f7-93c0-4c2b28562876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted tags and fill NA with forward fill\n",
    "to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n",
    "data['Tag'] = data['Tag'].apply(lambda x: [i if i not in to_remove else np.NaN for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8b9390-8e99-444d-9f63-684ebedec4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\AppData\\Local\\Temp\\ipykernel_14928\\3307683612.py:1: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data['Tag'] = data['Tag'].apply(lambda x: list(pd.Series(x).fillna(method='ffill')))\n",
      "C:\\Users\\Abhinav\\AppData\\Local\\Temp\\ipykernel_14928\\3307683612.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data['Tag'] = data['Tag'].apply(lambda x: list(pd.Series(x).fillna(method='bfill')))\n"
     ]
    }
   ],
   "source": [
    "data['Tag'] = data['Tag'].apply(lambda x: list(pd.Series(x).fillna(method='ffill')))\n",
    "data['Tag'] = data['Tag'].apply(lambda x: list(pd.Series(x).fillna(method='bfill')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b949e1c4-161c-4a51-bd82-6d28c0c898a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\AppData\\Local\\Temp\\ipykernel_14928\\746809385.py:1: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  pd.Series(data['Tag'].iloc[22]).fillna(method='bfill')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         O\n",
       "1         O\n",
       "2     B-per\n",
       "3     I-per\n",
       "4         O\n",
       "5         O\n",
       "6         O\n",
       "7         O\n",
       "8         O\n",
       "9         O\n",
       "10        O\n",
       "11        O\n",
       "12        O\n",
       "13        O\n",
       "14        O\n",
       "15        O\n",
       "16        O\n",
       "17        O\n",
       "18    B-tim\n",
       "19        O\n",
       "20        O\n",
       "21        O\n",
       "22        O\n",
       "23        O\n",
       "24        O\n",
       "25    B-geo\n",
       "26        O\n",
       "27        O\n",
       "28    B-geo\n",
       "29        O\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data['Tag'].iloc[22]).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122a683c-3cd9-4abe-8bc0-78801ccb8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id  = {str(k): v for v, k in enumerate(data['Tag'].explode().unique())}\n",
    "id2label = {v: str(k) for v, k in enumerate(data['Tag'].explode().unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f78bfb-0593-42b2-966c-c587bb157884",
   "metadata": {},
   "source": [
    "### Preparing the dataset and dataloaders\n",
    "\n",
    "Iterations: the number of batches needed to complete one Epoch \\\n",
    "Batch Size: The number of training samples used in one e iteration \\\n",
    "Epoch: one full cycle through the training dataset. A cycle is composed of many iterationsio\\\n",
    "Number of Steps per Epoch = (Total Number of Training Samples) / (Batch Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e27a7b-4a2b-4494-9750-d0199c5b5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\anaconda3\\envs\\ner\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "#Will be used in Gradient Clipping, to restrict gradient value below MAX_GRAD_NORM\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9ed854-fa1e-4e8a-a710-5855e3e2c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization and extending its label if token getting split\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed74393-06df-4654-b981-e6bdbe520a04",
   "metadata": {},
   "source": [
    "### Preparing Pytorch Dataset and DataLoader\n",
    "\n",
    "coverting data to pytorch tensors which can be feeded in Model.\n",
    "torch.utils.data.Dataset is an abtract class which should be inherited in every dataset class, to confirm if the class has\n",
    "1. __init__ - The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms\n",
    "2. __getitem__ -  returns a sample from the dataset at the given index idx, with label, in class we dont need to call object, can use direct slicing  [] (indexer) in object\n",
    "3. __len__ - so that len(dataset) returns the size of the dataset.\n",
    "\n",
    "\n",
    "Step to do:\n",
    "1. **wordpiece tokenization** : A tricky part of NER with BERT is that BERT relies on wordpiece tokenization, rather than **word tokenization**. This means that we should also define the labels at the wordpiece-level, rather than the word-level!\n",
    "\n",
    "For example, if you have word like \"Washington\" which is labeled as \"b-gpe\"\\\n",
    ", but it gets tokenized to \"Wash\", \"##ing\", \"##ton\", then we will have to propagate the word’s original label to all of it\\\n",
    "s wordpieces: \"b-gpe\", \"b-gpe\", \"b-gpe\". The model should be able to produce the correct labels for each individual wordpiece. The function below (taken from here) implements this.\n",
    "\n",
    "\n",
    "2. **Add '[CLS]' and '[SEP]' tags at front and end of every example** \\\n",
    "[SEP] is for separating sentences for the next sentence prediction task\\\n",
    "CLS stands for classification, we usually add these token at front and end of every sentence\n",
    "\n",
    "\n",
    "3. **Truncating and Padding** each example to MAX_Length, so that each row is equal, if one example length smaller than max_length, add padding with tag [PAD] with label 'O'. If example lenght is greater than max_lenght, truncate.\n",
    "\n",
    "4. **Add Attention mask** (for each token add 0(no attention) or 1(attention)) we can skip this, training will still be done, but model might not perform accurately, as it is taking padding layers in prediction, to avoid this, add attention mask to non-padded tokens, so that model pays more attention to non-padded words  \n",
    "\n",
    "5. **Tokens to Ids** in Tokenizer vocabulary each word is given one id, map each word ids in an example to its tokenizer id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9c19ae-d3c0-427d-843c-33558ad3fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.len = len(dataframe)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data['Sentence'].iloc[index]  \n",
    "        word_labels = self.data['Tag'].iloc[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b51bbe-5df0-4ad3-8845-7df7376d3900",
   "metadata": {},
   "source": [
    "### Splitting Training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13fc20b7-7218-4627-ba8f-708f4de07dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47959, 5)\n",
      "TRAIN Dataset: (38367, 5)\n",
      "TEST Dataset: (9592, 5)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850790a5-0f18-4fa7-af30-ff136300d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([  101,  1996,  2088,  2740,  3029,  2758,  8168,  1997,  1996,  9252,\n",
       "          1044,  2629,  2078,  2487, 10178,  2579,  2013,  2048,  4743, 19857,\n",
       "          5694,  1999,  4977,  2265,  1037,  7263,  7403,  2689,  1999,  1996,\n",
       "          7865,  1011,  1037,  3696,  2009,  2089,  2022, 14163, 29336,  2075,\n",
       "          2046,  1037,  2529, 19857,  7865,  2008,  2071,  3102,  8817,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([0, 0, 4, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195c2ea-5c8c-4c21-9b41-854cdefee0ab",
   "metadata": {},
   "source": [
    "### Check labels and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74aa2c49-f866-4dff-ab57-3bedf8d5fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       O\n",
      "the         O\n",
      "world       B-org\n",
      "health      I-org\n",
      "organization  I-org\n",
      "says        O\n",
      "samples     O\n",
      "of          O\n",
      "the         O\n",
      "deadly      O\n",
      "h           O\n",
      "##5         O\n",
      "##n         O\n",
      "##1         O\n",
      "strain      O\n",
      "taken       O\n",
      "from        O\n",
      "two         O\n",
      "bird        O\n",
      "flu         O\n",
      "victims     O\n",
      "in          O\n",
      "turkey      B-geo\n",
      "show        O\n",
      "a           O\n",
      "slight      O\n",
      "genetic     O\n",
      "change      O\n",
      "in          O\n",
      "the         O\n"
     ]
    }
   ],
   "source": [
    "# print the first 30 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[1][\"ids\"][:30]), training_set[1][\"targets\"][:30]):\n",
    "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1f09c-05c2-49e9-b86a-ca9d518cd301",
   "metadata": {},
   "source": [
    "### Setting Dataloader\n",
    "\n",
    "Dataloaders are iterables over the dataset. So when you iterate over it, it will return B randomly from the dataset collected samples (including the data-sample and the target/label), where B is the batch-size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8a776da-dfd3-463d-a5b5-a22296696b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 2,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 2,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c2bc2a-308d-427d-8259-e49b592ecbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x18a4176a4b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69e951dc-a58e-4590-aeef-861fcb930dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3e564-7a32-47e9-bf35-d9c223eda00c",
   "metadata": {},
   "source": [
    "### Sanity Check on Model\n",
    "\n",
    "The initial loss of your model should be close to -ln(1/number of classes) = -ln(1/1) = 2.39\\\n",
    "But WHY?\n",
    "In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/11. The loss for a given token will thus be -ln(1/11).\n",
    "\n",
    "\n",
    "**unsqueeze**: a new dimension of size 1 is inserted at the specified position, Always an unsqueeze operation increases the dimension of the output tensor. Here at d=0, new dimension is added\n",
    "\n",
    "**squeeze**: a new dimension of size 1 is removed at the specified position, Always an squeeze operation reduces the dimension of the output tensor. Here at d=0, dimension will be removed\n",
    "\n",
    "torch.to is used to send an object to same device, cuda or CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100e6cae-af2d-4b8d-8624-bd70e65b9855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9531, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e71342-6752-4cc3-b84b-966aabfb0ade",
   "metadata": {},
   "source": [
    "### Set Optimizer\n",
    "\n",
    "Adjust the parameters by the gradients collected in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2364dc10-6e64-472e-ad39-1894f7e8fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff42a34-df3d-4910-8ff1-3a07d55d2617",
   "metadata": {},
   "source": [
    "### How to do training, line by line explanation\n",
    "\n",
    "1. **Putting model in training mode**: This allows model to change inner layers like Droput of Batchnorm to update their gradients and weights\n",
    "2. **Looping through training_loader** which is DataLoader object by pytorch, it is an iterable which iterates in batches\n",
    "3. **Objects in Same Device** : send all objects to GPU or CPU\n",
    "4. **Get Predictions** on first batch, store in outputs\n",
    "5. **get loss value** and add it outside loop, also add number of batches in nb_tr_steps later\n",
    "6. **Print Loss** for every 100 batches calculate loss\n",
    "7. **Calculate Training Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbc86fbc-7f6b-4d2d-a813-53bf758f3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57af6fa6-0c96-419e-a203-d4d6b6c31129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 1.9192566871643066\n",
      "Training loss per 100 training steps: 0.3785911900830446\n",
      "Training loss per 100 training steps: 0.26043745762302506\n",
      "Training loss per 100 training steps: 0.20875351297734088\n",
      "Training loss per 100 training steps: 0.18239561040442148\n",
      "Training loss per 100 training steps: 0.16085621369482128\n",
      "Training loss per 100 training steps: 0.14644514254221502\n",
      "Training loss per 100 training steps: 0.13562645023547845\n",
      "Training loss per 100 training steps: 0.12526230064375365\n",
      "Training loss per 100 training steps: 0.11714277927816477\n",
      "Training loss per 100 training steps: 0.11137009966843106\n",
      "Training loss per 100 training steps: 0.10535901875692343\n",
      "Training loss per 100 training steps: 0.10044827782031347\n",
      "Training loss per 100 training steps: 0.09668423340369164\n",
      "Training loss per 100 training steps: 0.09316675889936955\n",
      "Training loss per 100 training steps: 0.09001713857508582\n",
      "Training loss per 100 training steps: 0.08677602677939368\n",
      "Training loss per 100 training steps: 0.08436460822543074\n",
      "Training loss per 100 training steps: 0.08174958627593178\n",
      "Training loss per 100 training steps: 0.07982999406727291\n",
      "Training loss per 100 training steps: 0.07761489927418705\n",
      "Training loss per 100 training steps: 0.0755833023832989\n",
      "Training loss per 100 training steps: 0.07406749304422922\n",
      "Training loss per 100 training steps: 0.07240069937740073\n",
      "Training loss per 100 training steps: 0.07098260337156381\n",
      "Training loss per 100 training steps: 0.06958693696958822\n",
      "Training loss per 100 training steps: 0.06836516247377099\n",
      "Training loss per 100 training steps: 0.06722599030189597\n",
      "Training loss per 100 training steps: 0.0661975842208275\n",
      "Training loss per 100 training steps: 0.06527760266835209\n",
      "Training loss per 100 training steps: 0.06461139366215189\n",
      "Training loss per 100 training steps: 0.06362340591152033\n",
      "Training loss per 100 training steps: 0.06269807893127786\n",
      "Training loss per 100 training steps: 0.06164397338232057\n",
      "Training loss per 100 training steps: 0.06089585068522445\n",
      "Training loss per 100 training steps: 0.06016560000964436\n",
      "Training loss per 100 training steps: 0.059484037416278154\n",
      "Training loss per 100 training steps: 0.058978172747804994\n",
      "Training loss per 100 training steps: 0.05851373899101818\n",
      "Training loss per 100 training steps: 0.057877068656846276\n",
      "Training loss per 100 training steps: 0.05742653866676141\n",
      "Training loss per 100 training steps: 0.05690933818609628\n",
      "Training loss per 100 training steps: 0.056286921945266244\n",
      "Training loss per 100 training steps: 0.05575089378868998\n",
      "Training loss per 100 training steps: 0.05514418795652784\n",
      "Training loss per 100 training steps: 0.054687316085698594\n",
      "Training loss per 100 training steps: 0.05411116213074874\n",
      "Training loss per 100 training steps: 0.05352256013561592\n",
      "Training loss per 100 training steps: 0.05316124632267648\n",
      "Training loss per 100 training steps: 0.05279438139997748\n",
      "Training loss per 100 training steps: 0.05253892189264083\n",
      "Training loss per 100 training steps: 0.0521239889968338\n",
      "Training loss per 100 training steps: 0.051747488193541184\n",
      "Training loss per 100 training steps: 0.05132834772148541\n",
      "Training loss per 100 training steps: 0.0509692925828861\n",
      "Training loss per 100 training steps: 0.05072055633219819\n",
      "Training loss per 100 training steps: 0.0502985638516515\n",
      "Training loss per 100 training steps: 0.05003552328548478\n",
      "Training loss per 100 training steps: 0.04966930588548187\n",
      "Training loss per 100 training steps: 0.0493874750375702\n",
      "Training loss per 100 training steps: 0.04904070198240516\n",
      "Training loss per 100 training steps: 0.0487920944563132\n",
      "Training loss per 100 training steps: 0.048480646146138444\n",
      "Training loss per 100 training steps: 0.048210547463107246\n",
      "Training loss per 100 training steps: 0.0480022523901689\n",
      "Training loss per 100 training steps: 0.04766462281846481\n",
      "Training loss per 100 training steps: 0.04743019672779708\n",
      "Training loss per 100 training steps: 0.04720163724160337\n",
      "Training loss per 100 training steps: 0.04689103621150814\n",
      "Training loss per 100 training steps: 0.04663578802074165\n",
      "Training loss per 100 training steps: 0.0463856903689714\n",
      "Training loss per 100 training steps: 0.04615354409582613\n",
      "Training loss per 100 training steps: 0.04589728424723929\n",
      "Training loss per 100 training steps: 0.04570597350216684\n",
      "Training loss per 100 training steps: 0.04553418677601902\n",
      "Training loss per 100 training steps: 0.045378117946198664\n",
      "Training loss per 100 training steps: 0.04521110899164426\n",
      "Training loss per 100 training steps: 0.04495813891867928\n",
      "Training loss per 100 training steps: 0.044742748586895226\n",
      "Training loss per 100 training steps: 0.044565590119730464\n",
      "Training loss per 100 training steps: 0.04441059418240957\n",
      "Training loss per 100 training steps: 0.04420497807239302\n",
      "Training loss per 100 training steps: 0.04398734373141965\n",
      "Training loss per 100 training steps: 0.04382587733286177\n",
      "Training loss per 100 training steps: 0.043626614530376966\n",
      "Training loss per 100 training steps: 0.0434840102766062\n",
      "Training loss per 100 training steps: 0.04338342266466154\n",
      "Training loss per 100 training steps: 0.04322428507792948\n",
      "Training loss per 100 training steps: 0.04313274497097883\n",
      "Training loss per 100 training steps: 0.04300668757689524\n",
      "Training loss per 100 training steps: 0.04288553065416135\n",
      "Training loss per 100 training steps: 0.04275118913673355\n",
      "Training loss per 100 training steps: 0.04256543366053382\n",
      "Training loss per 100 training steps: 0.04242412078104652\n",
      "Training loss per 100 training steps: 0.04231690828521181\n",
      "Training loss per 100 training steps: 0.042127342586707583\n",
      "Training loss per 100 training steps: 0.04193688138172169\n",
      "Training loss per 100 training steps: 0.041872268061733636\n",
      "Training loss per 100 training steps: 0.04172241704398525\n",
      "Training loss per 100 training steps: 0.04156712654940864\n",
      "Training loss per 100 training steps: 0.041437077828528936\n",
      "Training loss per 100 training steps: 0.0413898965207289\n",
      "Training loss per 100 training steps: 0.04120632901248346\n",
      "Training loss per 100 training steps: 0.04107536178172593\n",
      "Training loss per 100 training steps: 0.04091223909566941\n",
      "Training loss per 100 training steps: 0.040787629374495676\n",
      "Training loss per 100 training steps: 0.040680794770513035\n",
      "Training loss per 100 training steps: 0.04061889736519899\n",
      "Training loss per 100 training steps: 0.040587373644343235\n",
      "Training loss per 100 training steps: 0.04049461922941132\n",
      "Training loss per 100 training steps: 0.04038798884193022\n",
      "Training loss per 100 training steps: 0.04027976398702852\n",
      "Training loss per 100 training steps: 0.040188286432353316\n",
      "Training loss per 100 training steps: 0.04007854923628345\n",
      "Training loss per 100 training steps: 0.039949881479871996\n",
      "Training loss per 100 training steps: 0.039863300143223764\n",
      "Training loss per 100 training steps: 0.03976461830103321\n",
      "Training loss per 100 training steps: 0.039706352902068226\n",
      "Training loss per 100 training steps: 0.03961053461608642\n",
      "Training loss per 100 training steps: 0.039492493345151404\n",
      "Training loss per 100 training steps: 0.03938709613013729\n",
      "Training loss per 100 training steps: 0.039288195151976575\n",
      "Training loss per 100 training steps: 0.039260933020373735\n",
      "Training loss per 100 training steps: 0.03914746050914766\n",
      "Training loss per 100 training steps: 0.039078957880669424\n",
      "Training loss per 100 training steps: 0.03898046080802083\n",
      "Training loss per 100 training steps: 0.03885189225555511\n",
      "Training loss per 100 training steps: 0.038729386531341295\n",
      "Training loss per 100 training steps: 0.03864160426296656\n",
      "Training loss per 100 training steps: 0.038550287240991486\n",
      "Training loss per 100 training steps: 0.03850281545683643\n",
      "Training loss per 100 training steps: 0.03839750047859283\n",
      "Training loss per 100 training steps: 0.038364733536563195\n",
      "Training loss per 100 training steps: 0.038260004595831024\n",
      "Training loss per 100 training steps: 0.03814527907031167\n",
      "Training loss per 100 training steps: 0.03804326887515433\n",
      "Training loss per 100 training steps: 0.037944663942813164\n",
      "Training loss per 100 training steps: 0.03784390593835616\n",
      "Training loss per 100 training steps: 0.037737367233388136\n",
      "Training loss per 100 training steps: 0.03765792348442021\n",
      "Training loss per 100 training steps: 0.03758141085324435\n",
      "Training loss per 100 training steps: 0.03751869022663013\n",
      "Training loss per 100 training steps: 0.03743188850530988\n",
      "Training loss per 100 training steps: 0.037345689632754926\n",
      "Training loss per 100 training steps: 0.03729326154832771\n",
      "Training loss per 100 training steps: 0.03720844997132052\n",
      "Training loss per 100 training steps: 0.0371356511181366\n",
      "Training loss per 100 training steps: 0.03709788452354572\n",
      "Training loss per 100 training steps: 0.0370476791524019\n",
      "Training loss per 100 training steps: 0.03695806113235259\n",
      "Training loss per 100 training steps: 0.03689241328077266\n",
      "Training loss per 100 training steps: 0.03681718772814686\n",
      "Training loss per 100 training steps: 0.03674469147231722\n",
      "Training loss per 100 training steps: 0.036692440100267815\n",
      "Training loss per 100 training steps: 0.03663484629797209\n",
      "Training loss per 100 training steps: 0.03662322218207462\n",
      "Training loss per 100 training steps: 0.03652256805362674\n",
      "Training loss per 100 training steps: 0.036445921992325686\n",
      "Training loss per 100 training steps: 0.03637348843958638\n",
      "Training loss per 100 training steps: 0.03632216227729758\n",
      "Training loss per 100 training steps: 0.03629024377568259\n",
      "Training loss per 100 training steps: 0.0362167379579075\n",
      "Training loss per 100 training steps: 0.03617977728747862\n",
      "Training loss per 100 training steps: 0.03611846023984674\n",
      "Training loss per 100 training steps: 0.03605907905636556\n",
      "Training loss per 100 training steps: 0.03600843538435875\n",
      "Training loss per 100 training steps: 0.03597851570296132\n",
      "Training loss per 100 training steps: 0.03591531187707593\n",
      "Training loss per 100 training steps: 0.035853000040244484\n",
      "Training loss per 100 training steps: 0.035794828448926654\n",
      "Training loss per 100 training steps: 0.035764739868377186\n",
      "Training loss per 100 training steps: 0.035708354262698735\n",
      "Training loss per 100 training steps: 0.03564818866345712\n",
      "Training loss per 100 training steps: 0.035592195187613834\n",
      "Training loss per 100 training steps: 0.03554594124976426\n",
      "Training loss per 100 training steps: 0.035490823055191946\n",
      "Training loss per 100 training steps: 0.03546897052607874\n",
      "Training loss per 100 training steps: 0.035419891129147923\n",
      "Training loss per 100 training steps: 0.03535345066654577\n",
      "Training loss per 100 training steps: 0.035308406071761035\n",
      "Training loss per 100 training steps: 0.03524353261600142\n",
      "Training loss per 100 training steps: 0.03520205073091667\n",
      "Training loss per 100 training steps: 0.03514679946461679\n",
      "Training loss per 100 training steps: 0.035091574542642584\n",
      "Training loss per 100 training steps: 0.03502899090402538\n",
      "Training loss per 100 training steps: 0.03496767360692954\n",
      "Training loss per 100 training steps: 0.03493341349164918\n",
      "Training loss per 100 training steps: 0.03488360911982327\n",
      "Training loss per 100 training steps: 0.03482998232901478\n",
      "Training loss per 100 training steps: 0.034777378398870074\n",
      "Training loss per 100 training steps: 0.034722923709554425\n",
      "Training loss per 100 training steps: 0.03467839117312023\n",
      "Training loss epoch: 0.0346346201253451\n",
      "Training accuracy epoch: 0.953680371367989\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47629deb-9f18-4494-a6a2-ae27f30bfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83e9e407-a2c4-4787-971f-7271d76230fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0006900187581777573\n",
      "Validation loss per 100 evaluation steps: 0.026479551454301174\n",
      "Validation loss per 100 evaluation steps: 0.025055006116784156\n",
      "Validation loss per 100 evaluation steps: 0.025769941882309435\n",
      "Validation loss per 100 evaluation steps: 0.02606673402014781\n",
      "Validation loss per 100 evaluation steps: 0.025266974010295536\n",
      "Validation loss per 100 evaluation steps: 0.02546787863304444\n",
      "Validation loss per 100 evaluation steps: 0.025639767960001985\n",
      "Validation loss per 100 evaluation steps: 0.025875212580350228\n",
      "Validation loss per 100 evaluation steps: 0.025828052442327692\n",
      "Validation loss per 100 evaluation steps: 0.025343479142897272\n",
      "Validation loss per 100 evaluation steps: 0.02491398005335309\n",
      "Validation loss per 100 evaluation steps: 0.02464526916829511\n",
      "Validation loss per 100 evaluation steps: 0.02482229084847723\n",
      "Validation loss per 100 evaluation steps: 0.024937600665665523\n",
      "Validation loss per 100 evaluation steps: 0.025000778285623713\n",
      "Validation loss per 100 evaluation steps: 0.025050213206180578\n",
      "Validation loss per 100 evaluation steps: 0.024754258582750314\n",
      "Validation loss per 100 evaluation steps: 0.02470466041216371\n",
      "Validation loss per 100 evaluation steps: 0.024607204337918836\n",
      "Validation loss per 100 evaluation steps: 0.024766366671604742\n",
      "Validation loss per 100 evaluation steps: 0.02496946733206599\n",
      "Validation loss per 100 evaluation steps: 0.024872336487340614\n",
      "Validation loss per 100 evaluation steps: 0.02498090301975399\n",
      "Validation loss per 100 evaluation steps: 0.02500961359708974\n",
      "Validation loss per 100 evaluation steps: 0.025034734220602754\n",
      "Validation loss per 100 evaluation steps: 0.024954687490541253\n",
      "Validation loss per 100 evaluation steps: 0.02482881030171599\n",
      "Validation loss per 100 evaluation steps: 0.024939598982005404\n",
      "Validation loss per 100 evaluation steps: 0.024909580101000358\n",
      "Validation loss per 100 evaluation steps: 0.024796149759114323\n",
      "Validation loss per 100 evaluation steps: 0.025031500151384087\n",
      "Validation loss per 100 evaluation steps: 0.02510275422047624\n",
      "Validation loss per 100 evaluation steps: 0.025181192502472204\n",
      "Validation loss per 100 evaluation steps: 0.025179778516450167\n",
      "Validation loss per 100 evaluation steps: 0.025176465062702394\n",
      "Validation loss per 100 evaluation steps: 0.025237438786273846\n",
      "Validation loss per 100 evaluation steps: 0.025215630796058066\n",
      "Validation loss per 100 evaluation steps: 0.025153795488709076\n",
      "Validation loss per 100 evaluation steps: 0.025103122843271878\n",
      "Validation loss per 100 evaluation steps: 0.025041799710657345\n",
      "Validation loss per 100 evaluation steps: 0.02501880661052818\n",
      "Validation loss per 100 evaluation steps: 0.024887948868027254\n",
      "Validation loss per 100 evaluation steps: 0.02501874126348168\n",
      "Validation loss per 100 evaluation steps: 0.025122600134565652\n",
      "Validation loss per 100 evaluation steps: 0.025067105439129184\n",
      "Validation loss per 100 evaluation steps: 0.025104673386956426\n",
      "Validation loss per 100 evaluation steps: 0.025012311115584388\n",
      "Validation Loss: 0.024989265702057156\n",
      "Validation Accuracy: 0.9623658991015852\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bd0b769-9748-48ff-800e-be1acc9e66d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.81      0.90      0.85     11331\n",
      "         gpe       0.95      0.93      0.94      3380\n",
      "         org       0.73      0.59      0.66      6642\n",
      "         per       0.77      0.80      0.79      5366\n",
      "         tim       0.88      0.81      0.84      4439\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     31158\n",
      "   macro avg       0.83      0.81      0.81     31158\n",
      "weighted avg       0.81      0.81      0.81     31158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820c87b-0baf-4295-9503-9318d9b2f6de",
   "metadata": {},
   "source": [
    "### Comparing with Existing and Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e8fffe-62f6-4e6c-8fcb-2f5e587d6461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Untrained_model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "Untrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adc9753b-c205-4297-8d74-d34aeb1f5fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'geo',\n",
       "  'score': 0.13686256,\n",
       "  'word': 'my',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'tim',\n",
       "  'score': 0.12575148,\n",
       "  'word': 'name',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'geo',\n",
       "  'score': 0.13910821,\n",
       "  'word': 'is',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'tim',\n",
       "  'score': 0.17786944,\n",
       "  'word': 'niels and',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'geo',\n",
       "  'score': 0.13867724,\n",
       "  'word': 'new',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'tim',\n",
       "  'score': 0.14464562,\n",
       "  'word': 'york',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'tim',\n",
       "  'score': 0.21768254,\n",
       "  'word': 'is',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'tim',\n",
       "  'score': 0.12822677,\n",
       "  'word': 'a',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'gpe',\n",
       "  'score': 0.15759417,\n",
       "  'word': 'city',\n",
       "  'start': None,\n",
       "  'end': None}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\", model=Untrained_model.to(device), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "pipe(\"My name is Niels and New York is a city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "435f0f02-b3b4-4c85-9e8a-f246b9da335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'per',\n",
       "  'score': 0.673212,\n",
       "  'word': 'ni',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'per',\n",
       "  'score': 0.70676476,\n",
       "  'word': '##els',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity_group': 'geo',\n",
       "  'score': 0.9735999,\n",
       "  'word': 'new york',\n",
       "  'start': None,\n",
       "  'end': None}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(task=\"token-classification\", model=model.to(device), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "pipe(\"My name is Niels and New York is a city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0c605-9d27-4fa0-9a08-247108487f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603ca30-8144-4955-bf44-6e59be7d6ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ce6ab-c56f-47b3-b23f-3e818ca49ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151db784-cb37-404a-b990-67b3936470f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
